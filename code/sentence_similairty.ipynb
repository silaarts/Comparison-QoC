{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few sentences to encode (sentences **0** and **2** are both similar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "        \n",
    "df = pd.read_csv('preprocessed.csv')\n",
    "df = df[df[\"sentence\"].str.len() < 512] \n",
    "\n",
    "display(df)\n",
    "\n",
    "sentences = list(df[\"sentence\"])\n",
    "precursors = list(df[\"precursor\"])\n",
    "# succesors = list(df[\"succesor\"])\n",
    "\n",
    "# ----------------\n",
    "# preprocessing\n",
    "# ----------------\n",
    "\n",
    "sentences = [re.sub(\"(b|z|f|i) \", \" \", sentence) for sentence in sentences]\n",
    "sentences = [re.sub(\"sp[0-9]+ \", \" \", sentence) for sentence in sentences]\n",
    "sentences = [re.sub(\"(\\n|\\r)+ \", \" \", sentence) for sentence in sentences]\n",
    "\n",
    "display(sentences[0:5])\n",
    "\n",
    "flat_sentences = sentences\n",
    "sentences = list(chunks(sentences, 256))\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "# -------------\n",
    "\n",
    "precursors = [re.sub(\"(b|z|f|i) \", \" \", sentence) for sentence in precursors]\n",
    "precursors = [re.sub(\"sp[0-9]+ \", \" \", sentence) for sentence in precursors]\n",
    "precursors = [re.sub(\"(\\n|\\r)+ \", \" \", sentence) for sentence in precursors]\n",
    "\n",
    "display(precursors[0:5])\n",
    "\n",
    "flat_precursors = precursors\n",
    "precursors = list(chunks(precursors, 256))\n",
    "\n",
    "# -------------\n",
    "\n",
    "# select label columns\n",
    "cols = df.columns\n",
    "label_cols = list(cols[3:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "classes = label_cols\n",
    "\n",
    "# set header for all label columns\n",
    "df['labels'] = list(df[label_cols].values)\n",
    "display(df.head())\n",
    "\n",
    "# get input and outputs\n",
    "labels = list(df.labels.values)\n",
    "display(labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip3 install sentence_transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "sentence_embeddings = np.zeros((0, 768))\n",
    "\n",
    "for idx in trange(len(sentences)):\n",
    "    lines = sentences[idx]\n",
    "    new_val = model.encode(lines)\n",
    "    # print(np.shape(new_val))\n",
    "    sentence_embeddings = np.concatenate((sentence_embeddings, new_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precursors_embeddings = np.zeros((0, 768))\n",
    "\n",
    "for idx in trange(len(sentences)):\n",
    "    lines = precursors[idx]\n",
    "    new_val = model.encode(lines)\n",
    "    # print(np.shape(new_val))\n",
    "    precursors_embeddings = np.concatenate((precursors_embeddings, new_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_embeddings.shape)\n",
    "print(precursors_embeddings.shape)\n",
    "\n",
    "full_embeddings = np.concatenate((sentence_embeddings, precursors_embeddings), axis=1)\n",
    "full_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sims = cosine_similarity(\n",
    "    sentence_embeddings,\n",
    "    sentence_embeddings\n",
    ")\n",
    "\n",
    "precursors_sims = cosine_similarity(\n",
    "    precursors_embeddings,\n",
    "    precursors_embeddings\n",
    ")\n",
    "\n",
    "full_sims = cosine_similarity(\n",
    "    full_embeddings,\n",
    "    full_embeddings\n",
    ")\n",
    "\n",
    "print(np.shape(sims))\n",
    "\n",
    "print(sims[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def n_most_similar(similarities, sentences, n):\n",
    "\n",
    "    most_similar_idx = np.argsort(similarities,  )[::-1][:n]\n",
    "    \n",
    "    return [(sentences[idx], similarities[idx]) for idx in most_similar_idx]\n",
    "    \n",
    "print(flat_sentences[0])\n",
    "n_most_similar(sims[0], flat_sentences, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_most_similar(similarities, sentences, labels, cutoff):\n",
    "\n",
    "    similar_idx = [[sentences[idx], labels[idx], sim] for sim, idx in zip(similarities, range(len(similarities))) if sim > cutoff]\n",
    "    \n",
    "    return similar_idx\n",
    "  \n",
    "print(flat_sentences[1])\n",
    "\n",
    "sim_result = cutoff_most_similar(full_sims[1], flat_sentences, labels, 0.8)\n",
    "\n",
    "display(sim_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_result = cutoff_most_similar(sims[1], flat_sentences, labels, 0.8)\n",
    "\n",
    "display(sim_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, e\n",
    "\n",
    "def entropy(labels, base=None):\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    value,counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts / n_labels\n",
    "    n_classes = np.count_nonzero(probs)\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "\n",
    "    ent = 0.\n",
    "\n",
    "    # Compute entropy\n",
    "    base = e if base is None else base\n",
    "    for i in probs:\n",
    "        ent -= i * log(i, base)\n",
    "\n",
    "    return ent\n",
    "\n",
    "def calc_error(label, other_labels):\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "\n",
    "    errors = []\n",
    "    \n",
    "    for other_label in other_labels.transpose():\n",
    "        # optimistic_metric = np.multiply(label, other_label)\n",
    "        # pessimistic_metric = np.logical_xor(label, other_label)\n",
    "        # realistic_metric = np.abs(np.subtract(label, other_label))\n",
    "        realistic_metric = np.power(np.subtract(label, other_label), 100)\n",
    "        # errors.append(np.multiply(label, other_label))\n",
    "        errors.append(realistic_metric)\n",
    "\n",
    "    return np.array(errors, dtype=float)\n",
    "\n",
    "\n",
    "def entropy_cutoff_most_similar(similarities, sentences, labels, cutoff):\n",
    "    similar_idx = [[sentences[idx], labels[idx], sim] for sim, idx in zip(similarities, range(len(similarities))) if sim > cutoff]\n",
    "    \n",
    "    similar_labels = np.transpose(np.array([s[1] for s in similar_idx]))\n",
    "    \n",
    "    return [1 - entropy(label) for label in similar_labels], len(similar_idx)\n",
    "\n",
    "# rules\n",
    "# the similarity of a text sentence should fall in the domain of [0, 1]\n",
    "# any arbitrary pair of sentence s and p are always 100% similar to each other, iff s.text = p.text and s.label = p.label\n",
    "# for any arbitrary pair of sentence s and p, similarity(s, p) = similarity(s, p)\n",
    "# if s.label = p.label and text_similarity(s, p) = 0, then similarity(s, p) = 0\n",
    "# if s.label != p.label and text_similarity(s, p) = 1, then similarity(s, p) = 0\n",
    "# if s.label != p.label and text_similarity(s, p) = 0, then similarity(s, p) = 1\n",
    "def error_similar(idx, similarities, sentences, labels):\n",
    "    similar_idx = [[sentences[idx], labels[idx], sim] for sim, idx in zip(similarities, range(len(similarities)))]\n",
    "    \n",
    "    similar_labels = np.transpose(np.array([s[1] for s in similar_idx]))\n",
    "    \n",
    "    error = calc_error(similar_labels[:,idx], similar_labels)\n",
    "    # print(error)\n",
    "    # print(error[:,1])\n",
    "    # test_sim = np.full((4), 1 - similarities[idx])\n",
    "    # print(error[idx,:], test_sim, np.subtract(test_sim, error[idx,:])) # should be [1, 1, ...., 1]\n",
    "    # print(error[idx,:], similarities[idx], np.abs(np.subtract(1 - similarities[idx], error[idx,:]))) # should be [0, 0, ...., 0]\n",
    "    for i in range(len(error[0])):\n",
    "        error[:,i] = np.abs(np.subtract(1 - similarities, error[:,i]))\n",
    "    \n",
    "    return error\n",
    "\n",
    "\n",
    "print(flat_sentences[1])\n",
    "\n",
    "sim_result = np.mean(error_similar(1, sims[1], flat_sentences, labels), axis=0)\n",
    "sim_result_mirrored = np.maximum(sim_result, 1 - sim_result)\n",
    "# sim_result = error_similar(sims[1], flat_sentences, labels)\n",
    "\n",
    "display(sim_result_mirrored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement = np.zeros((1, len(labels[0])))\n",
    "\n",
    "for i in trange(len(sims)):\n",
    "    sim = sims[i]\n",
    "    precursors_sim = precursors_sims[i]\n",
    "    full_sim = full_sims[i]\n",
    "    error = np.mean(error_similar(0, full_sim, flat_sentences, labels), axis=0)\n",
    "    error_corrected = np.maximum(error, 1 - error)\n",
    "    agreement += error_corrected * (len(sims) - i)\n",
    "    \n",
    "agreement /= len(sims) * len(sims) / 2\n",
    "\n",
    "display(agreement)\n",
    "display(np.mean(agreement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.loadtxt(\"predictions.csv\", delimiter = \",\")\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_agreement = np.zeros((1, len(labels[0])))\n",
    "\n",
    "for i in trange(len(sims)):\n",
    "    sim = sims[i]\n",
    "    precursors_sim = precursors_sims[i]\n",
    "    full_sim = full_sims[i]\n",
    "    error = np.mean(error_similar(0, full_sim, flat_sentences, pred_labels), axis=0)\n",
    "    error_corrected = np.maximum(error, 1 - error)\n",
    "    pred_agreement += error_corrected * (len(sims) - i)\n",
    "    \n",
    "pred_agreement /= len(sims) * len(sims) / 2\n",
    "\n",
    "display(pred_agreement)\n",
    "display(np.mean(pred_agreement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "table_classes = [\"Experienced QoC\", \"Experiences\", \"Expectations\", \"Context\"]\n",
    "table_classes = np.append(table_classes, 'Average')\n",
    "table_manual = np.append(agreement[0], np.mean(agreement[0])) * 100\n",
    "table_digital = np.append(pred_agreement[0], np.mean(pred_agreement[0])) * 100\n",
    "\n",
    "table_manual = np.round(table_manual, 1)\n",
    "table_digital = np.round(table_digital, 1)\n",
    "\n",
    "print(table_manual)\n",
    "print(table_digital)\n",
    "\n",
    "table = np.transpose([table_classes, table_manual, table_digital])\n",
    "print(table)\n",
    "\n",
    "print(tabulate(table, headers=[\"Theme\", 'Manual (%)', 'Digital (%)'], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
