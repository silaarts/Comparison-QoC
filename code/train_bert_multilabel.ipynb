{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030484a3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4830c0f3",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting openpyxl\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m242.1/242.1 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\n",
      "\u001B[0mSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff64dbed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pdelobelle/robbert-v2-dutch-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1204,  0.2089, -0.0885, -0.1668,  0.2153,  0.1966, -0.2311,  0.0215,\n",
       "          0.0377,  0.1871, -0.0138,  0.0352, -0.0158, -0.1426, -0.0133]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocess_indexqual import preprocess, process_segment\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import RobertaModel, RobertaForSequenceClassification, RobertaTokenizer, AdamW, RobertaForQuestionAnswering\n",
    "from tqdm import tqdm, trange\n",
    "import gc\n",
    "import os\n",
    "from balance_sampler import MultilabelBalancedRandomSampler\n",
    "import shutil\n",
    "\n",
    "# use CUDA when available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using:\", device)\n",
    "\n",
    "# clean\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# define max length\n",
    "max_length = 512\n",
    "\n",
    "\n",
    "class MultilabelClassification(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels, hidden_dropout_prob=0.1, hidden_size=768):\n",
    "        super(MultilabelClassification, self).__init__()\n",
    "\n",
    "        self.config = {}\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.double_dense = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_size * 2, num_labels)\n",
    "\n",
    "    def forward(self, a_input_ids, a_input_mask, b_input_ids, b_input_mask):\n",
    "        x1 = self.bert_model(a_input_ids, attention_mask=a_input_mask)\n",
    "        x2 = self.bert_model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "        x1 = self.dropout(x1.last_hidden_state)\n",
    "        x1 = x1[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "\n",
    "        x2 = self.dropout(x2.last_hidden_state)\n",
    "        x2 = x2[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        x = self.double_dense(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    bert = RobertaModel.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", add_pooling_layer=False)\n",
    "\n",
    "    for param in bert.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = MultilabelClassification(bert, 15)\n",
    "    return model\n",
    "\n",
    "\n",
    "# TODO remove testing code\n",
    "tokenizer = RobertaTokenizer.from_pretrained('pdelobelle/robbert-v2-dutch-base')  # tokenizer\n",
    "context = tokenizer(\"Hallo, ik ben coen\", truncation=True,\n",
    "                                    max_length=max_length,\n",
    "                                    padding=True, return_tensors='pt')\n",
    "sentence = tokenizer(\"Ik vind het hier fijn, omdat ze goed verzorgen\", truncation=True,\n",
    "                                    max_length=max_length,\n",
    "                                    padding=True, return_tensors='pt')\n",
    "test_model = model_init()\n",
    "# print(test_model)\n",
    "test_model(context.input_ids, context.attention_mask, sentence.input_ids, sentence.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c60cb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ca816",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print start info\n",
    "print(\"starting to train BERT with INDEXQUAL data\")\n",
    "print(\"preprocessing ...\")\n",
    "\n",
    "from_cache = True\n",
    "\n",
    "# load preprocessed data\n",
    "if from_cache:\n",
    "    df = pd.read_csv('preprocessed.csv')\n",
    "    print(len(df))\n",
    "else:\n",
    "    classes, df = preprocess(mode=\"\")\n",
    "\n",
    "    # to csv\n",
    "    df.to_csv('preprocessed.csv', index = False, header=True)\n",
    "    \n",
    "# show data\n",
    "print(df.head())\n",
    "\n",
    "# select label columns\n",
    "cols = df.columns\n",
    "label_cols = list(cols[3:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)\n",
    "classes = label_cols\n",
    "\n",
    "# set header for all label columns\n",
    "df['labels'] = list(df[label_cols].values)\n",
    "df.head()\n",
    "\n",
    "# get input and outputs\n",
    "labels = list(df.labels.values)\n",
    "sentences = list(df.sentence.values)\n",
    "print(len(sentences))\n",
    "contexts = list(df.precursor.values)\n",
    "print(len(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d5c6ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n",
      "tokenizer outputs:  dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokenizer = RobertaTokenizer.from_pretrained('pdelobelle/robbert-v2-dutch-base')  # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(sentences, truncation=True,\n",
    "                                    max_length=max_length,\n",
    "                                    padding=True)\n",
    "print('tokenizer outputs: ', encodings.keys())\n",
    "\n",
    "context_encodings = tokenizer.batch_encode_plus(contexts, truncation=True,\n",
    "                                    max_length=max_length,\n",
    "                                    padding=True)\n",
    "print('tokenizer outputs: ', context_encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808075d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# preparing data format for training\n",
    "input_ids = encodings['input_ids']  # tokenized and encoded sentences\n",
    "attention_masks = encodings['attention_mask']  # attention masks\n",
    "context_input_ids = context_encodings['input_ids']  # tokenized and encoded sentences\n",
    "context_attention_masks = context_encodings['attention_mask']  # attention masks\n",
    "print(len(input_ids))\n",
    "print(len(attention_masks))\n",
    "print(len(context_input_ids))\n",
    "print(len(context_attention_masks))\n",
    "print(len(labels))\n",
    "\n",
    "# Identifying all samples that include only one label, this avoids issues with stratification\n",
    "label_counts = df.labels.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts == 1].keys()\n",
    "one_freq_idxs = sorted(list(df[df.labels.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)\n",
    "\n",
    "# Gathering single instance inputs to force into the training set after stratified split\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_context_ids = [context_input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_context_attention_masks = [context_attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]\n",
    "\n",
    "# Use train_test_split to split our data into train and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks, train_context_inputs, validation_context_inputs, train_context_masks, validation_context_masks = train_test_split(\n",
    "        input_ids, labels, attention_masks, context_input_ids, context_attention_masks,\n",
    "        random_state=2020, test_size=0.15, stratify=labels)\n",
    "\n",
    "# Add one frequency data to train data\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "train_context_inputs.extend(one_freq_context_ids)\n",
    "train_context_masks.extend(one_freq_context_attention_masks)\n",
    "train_labels.extend(one_freq_labels)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "train_context_inputs = torch.tensor(train_context_inputs)\n",
    "train_context_masks = torch.tensor(train_context_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "validation_context_inputs = torch.tensor(validation_context_inputs)\n",
    "validation_context_masks = torch.tensor(validation_context_masks)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0387148d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([936, 15])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff77d99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48,\n",
    "# or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 8\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because,\n",
    "# unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_context_inputs, train_context_masks, train_labels)\n",
    "train_sampler = MultilabelBalancedRandomSampler(train_labels)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_context_inputs, validation_context_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_labels)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e3e82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba1048",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Init model\n",
    "model = model_init()\n",
    "model.to(device)\n",
    "    \n",
    "# TODO add seed constant\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "005bef7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    best_name = 'bert_model_multi_label_indexqual_' + str(val_f1_accuracy)\n",
    "    dic = zip(range(0, len(classes)), classes)\n",
    "    torch.save(model.state_dict(), best_name)\n",
    "    \n",
    "    return best_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb4e1426",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "train_loss_per_epoch = []\n",
    "valid_loss_per_epoch = []\n",
    "valid_acc_set = []\n",
    "best_valid_f1 = 0\n",
    "best_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242c6aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0  # running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True) as pbar:\n",
    "        for i, batch in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, c_input_ids, c_input_mask, b_labels = batch\n",
    "\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for multilabel classification\n",
    "            logits = model(c_input_ids, c_input_mask, b_input_ids, b_input_mask)\n",
    "            loss_func = BCEWithLogitsLoss()\n",
    "\n",
    "            loss = loss_func(logits.view(-1, num_labels),\n",
    "                             b_labels.type_as(logits).view(-1, num_labels))  # convert labels to float for calculation\n",
    "            train_loss_set.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "            # update progress bar\n",
    "            pbar.update()\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
    "    train_loss_per_epoch.append(tr_loss / nb_tr_steps)\n",
    "    \n",
    "    ###############################################################################\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    logit_preds, true_labels, pred_labels, tokenized_texts = [], [], [], []\n",
    "\n",
    "    # Tracking variables\n",
    "    vd_loss = 0  # running loss\n",
    "    nb_vd_steps = 0\n",
    "    \n",
    "    # Predict\n",
    "    with tqdm(total=len(validation_dataloader), position=0, leave=True) as pbar:\n",
    "        for i, batch in enumerate(tqdm(validation_dataloader, position=0, leave=True)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, c_input_ids, c_input_mask, b_labels = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                b_logit_pred = model( c_input_ids, c_input_mask, b_input_ids, b_input_mask)\n",
    "\n",
    "                loss = loss_func(b_logit_pred.view(-1, num_labels),\n",
    "                     b_labels.type_as(b_logit_pred).view(-1, num_labels))  # convert labels to float for calculation\n",
    "                vd_loss += loss.item()\n",
    "                nb_vd_steps += 1\n",
    "\n",
    "                pred_label = torch.sigmoid(b_logit_pred)\n",
    "                b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "                pred_label = pred_label.to('cpu').numpy()\n",
    "                b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tokenized_texts.append(b_input_ids)\n",
    "            logit_preds.append(b_logit_pred)\n",
    "            true_labels.append(b_labels)\n",
    "            pred_labels.append(pred_label)\n",
    "            \n",
    "            # update progress bar\n",
    "            pbar.update()\n",
    "    \n",
    "    print(\"Validation loss: {}\".format(vd_loss / nb_vd_steps))\n",
    "    valid_loss_per_epoch.append(vd_loss / nb_vd_steps)\n",
    "        \n",
    "    # Flatten outputs\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    threshold = 0.50\n",
    "    pred_bools = [pl > threshold for pl in pred_labels]\n",
    "    true_bools = [tl == 1 for tl in true_labels]\n",
    "    val_f1_accuracy = f1_score(true_bools, pred_bools, average='micro') * 100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools) * 100\n",
    "\n",
    "    valid_acc_set.append(val_f1_accuracy)\n",
    "    print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "    print('Flat Validation Accuracy: ', val_flat_accuracy)   \n",
    "    \n",
    "    if val_f1_accuracy > best_valid_f1:\n",
    "        if os.path.exists(best_name) and os.path.isdir(best_name):\n",
    "            shutil.rmtree(best_name)\n",
    "        best_valid_f1 = val_f1_accuracy\n",
    "        best_name = save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948df9c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Simple predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ca8069",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# nlp = pipeline('text-classification', tokenizer=\"pdelobelle/robbert-v2-dutch-base\", model=best_name, return_all_scores=True)\n",
    "# model = RobertaForSequenceClassification.from_pretrained(best_name, num_labels=len(classes))\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d705ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(nlp(\"Ik verwacht dat ik geholpen wordt\")) # expectations\n",
    "# print(nlp(\"Het is toch kleinschalig wonen.\")) # context\n",
    "# print(nlp(\"hij komt van herstelzorg.\")) # past experiences\n",
    "# print(nlp(\"Ik denk dat de activiteiten misschien dat hier aangeboden zijn niet zo zijn als dat ik gedacht had..\")) # expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea216a7d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de1d3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "# Variables to gather full output\n",
    "logit_preds, true_labels, pred_labels, tokenized_texts = [], [], [], []\n",
    "\n",
    "# Use original distribution for evaluation (instead of a balanced distribution)\n",
    "validation_sampler = SequentialSampler(validation_labels)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "# Predict\n",
    "for i, batch in enumerate(validation_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, c_input_ids, c_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        b_logit_pred = model( c_input_ids, c_input_mask, b_input_ids, b_input_mask)\n",
    "        pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "        pred_label = pred_label.to('cpu').numpy()\n",
    "        b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "# Flatten outputs\n",
    "true_labels = [item for sublist in true_labels for item in sublist]\n",
    "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "\n",
    "# Calculate Accuracy\n",
    "threshold = 0.50\n",
    "pred_bools = [pl > threshold for pl in pred_labels]\n",
    "true_bools = [tl == 1 for tl in true_labels]\n",
    "val_f1_accuracy = f1_score(true_bools, pred_bools, average='micro') * 100\n",
    "val_flat_accuracy = accuracy_score(true_bools, pred_bools) * 100\n",
    "\n",
    "print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "print('Flat Validation Accuracy: ', val_flat_accuracy)   \n",
    "\n",
    "# calculate predicted class for single-label CFM\n",
    "true_labels_single = np.argmax(true_labels, axis=1)\n",
    "pred_labels_single = np.argmax(pred_labels, axis=1)\n",
    "\n",
    "cm = confusion_matrix(true_labels_single, pred_labels_single)\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "cm_df.index.name = 'Actual'\n",
    "cm_df.columns.name = 'Predicted'\n",
    "\n",
    "print(cm_df)\n",
    "\n",
    "# calculate predicted class for multi-label CFM\n",
    "true_labels_multi = np.round(true_labels)\n",
    "pred_labels_multi = np.round(pred_labels)\n",
    "\n",
    "cm_multi = multilabel_confusion_matrix(true_labels_multi, pred_labels_multi)\n",
    "\n",
    "for i in range(0, len(classes)):\n",
    "    cm_multi[i] = np.fliplr(np.rot90(cm_multi[i]))\n",
    "\n",
    "print(cm_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "figsize = (20, 8)\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_per_epoch, 'g')\n",
    "plt.plot(valid_loss_per_epoch, 'b')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm_df, annot=cm, fmt='', cmap=\"Blues\", annot_kws={\"size\": 10})\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "341c4d70e78eb4a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "def calc_accuracy(i):\n",
    "    true_1 = cm_multi[i,0,0] + cm_multi[i,1,1]\n",
    "    false_1 = cm_multi[i,1,0] + cm_multi[i,0,1]\n",
    "    return true_1 / (true_1 + false_1)\n",
    "\n",
    "def calc_sensitivity(i):\n",
    "    tp = cm_multi[i,1,1]\n",
    "    fn = cm_multi[i,1,0]\n",
    "    return round(tp / (tp + fn), 2)\n",
    "\n",
    "def calc_specificity(i):\n",
    "    tn = cm_multi[i,0,0]\n",
    "    fp = cm_multi[i,0,1]\n",
    "    return round(tn / (tn + fp), 2)\n",
    "\n",
    "tick_labels = ['Present', 'Absent']\n",
    "xlabel = \"Manually coded\"\n",
    "ylabel = \"As predicted by text mining\"\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(cm_multi[0], annot=cm_multi[0], fmt='', cmap=\"Blues\", annot_kws={\"size\": 15}, cbar=False, xticklabels=tick_labels, yticklabels=tick_labels)\n",
    "# axs[0, 0].set_title(classes[0] + \" (\" + str(calc_sensitivity(0)) + \", \" + str(calc_specificity(0)) + \")\")\n",
    "axs[0, 0].set_title('Experienced QoC', fontsize=20, fontweight='bold')\n",
    "plt.xlabel(xlabel, fontsize=12)\n",
    "plt.ylabel(ylabel, fontsize=12)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(cm_multi[1], annot=cm_multi[1], fmt='', cmap=\"Blues\", annot_kws={\"size\": 15}, cbar=False, xticklabels=tick_labels, yticklabels=tick_labels)\n",
    "# axs[0, 1].set_title(classes[1] + \" (\" + str(calc_sensitivity(1)) + \", \" + str(calc_specificity(1)) + \")\")\n",
    "axs[0, 1].set_title('Experiences', fontsize=20, fontweight='bold')\n",
    "plt.xlabel(xlabel, fontsize=12)\n",
    "plt.ylabel(ylabel, fontsize=12)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.heatmap(cm_multi[2], annot=cm_multi[2], fmt='', cmap=\"Blues\", annot_kws={\"size\": 15}, cbar=False, xticklabels=tick_labels, yticklabels=tick_labels)\n",
    "# axs[1, 0].set_title(classes[2] + \" (\" + str(calc_sensitivity(2)) + \", \" + str(calc_specificity(2)) + \")\")\n",
    "axs[1, 0].set_title('Expectations', fontsize=20, fontweight='bold')\n",
    "plt.xlabel(xlabel, fontsize=12)\n",
    "plt.ylabel(ylabel, fontsize=12)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.heatmap(cm_multi[3], annot=cm_multi[3], fmt='', cmap=\"Blues\", annot_kws={\"size\": 15}, cbar=False, xticklabels=tick_labels, yticklabels=tick_labels)\n",
    "# axs[1, 1].set_title(classes[3] + \" (\" + str(calc_sensitivity(3)) + \", \" + str(calc_specificity(3)) + \")\")\n",
    "axs[1, 1].set_title('Context', fontsize=20, fontweight='bold')\n",
    "plt.xlabel(xlabel, fontsize=12)\n",
    "plt.ylabel(ylabel, fontsize=12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "af40762d426fcbd7"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "affbd53c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m obj_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# with open(filepath, encoding='cp1252') as f:\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     11\u001B[0m     lines \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mreadlines()\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(lines)):\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data.txt'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filepath = 'data.txt'\n",
    "\n",
    "context_text = ''\n",
    "\n",
    "obj_list = []\n",
    "\n",
    "# with open(filepath, encoding='cp1252') as f:\n",
    "with open(filepath) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for i in range(0, len(lines)):\n",
    "        context = tokenizer(context_text, truncation=True,\n",
    "                                        max_length=max_length,\n",
    "                                        padding=True, return_tensors='pt')\n",
    "        sentence = tokenizer(lines[i], truncation=True,\n",
    "                                        max_length=max_length,\n",
    "                                        padding=True, return_tensors='pt')\n",
    "\n",
    "        context.to(device)\n",
    "        sentence.to(device)\n",
    "        \n",
    "        result = model(context.input_ids, context.attention_mask, sentence.input_ids, sentence.attention_mask)\n",
    "        \n",
    "        pred_bools = [pl > threshold for pl in result.detach().cpu().numpy()][0]\n",
    "        \n",
    "        obj = {\n",
    "            \"text\": lines[i],\n",
    "            \"labels\": pred_bools.tolist()\n",
    "        }\n",
    "        \n",
    "        obj_list.append(obj)\n",
    "\n",
    "        context_text += ' ' + lines[i]\n",
    "        \n",
    "        context_split = context_text.split(' ')\n",
    "        if len(context_split) > 511:\n",
    "            context_split = context_split[-511:]\n",
    "            context_text = ' '.join(context_split)\n",
    "            \n",
    "        \n",
    "json_dump = json.dumps(obj_list)\n",
    "\n",
    "print(json_dump)\n",
    "\n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    f.write(json_dump)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acec6a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
